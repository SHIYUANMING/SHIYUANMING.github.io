<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>SI251: Convex Optimization</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Yuanming Shi</div>
<div class="menu-item"><a href="home.html">Home</a></div>
<div class="menu-item"><a href="education.html">Education</a></div>
<div class="menu-item"><a href="research.html">Research</a></div>
<div class="menu-item"><a href="group.html">Group</a></div>
<div class="menu-item"><a href="teaching.html" class="current">Teaching</a></div>
<div class="menu-item"><a href="publicationsyear.html">Publicationsyear&nbsp;(by&nbsp;year)</a></div>
<div class="menu-item"><a href="publicationstopic.html">Publications&nbsp;(by&nbsp;topic)</a></div>
<div class="menu-item"><a href="talks.html">Talks</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>SI251: Convex Optimization</h1>
<div id="subtitle"><a href="http://shiyuanming.github.io">Yuanming Shi</a>, ShanghaiTech University, Spring 2018</div>
</div>
<h2>Description</h2>
<p>The focus of this course is theory and algorithms for convex optimization (though we also may touch upon nonconvex optimization problems at some points), with particular emphasis on problems that arise in data science and machine learning. Our goal is to gain a fundamental understanding of convex analysis, modeling and approximation, in addition with convergence rates, complexity, scaling and statistical behaviors of various algorithms, including both first-order and second-order methods, as well as both deterministic and randomized algorithms. Upon completing the course, students should have the capability of unveiling the hidden convexity of problems by appropriate manipulations, characterizing the solutions either analytically or algorithmically, designing and implementing efficient algorithms.</p>
<h2>Textbooks and Optional References</h2>
<p><b>Textbooks:</b></p>
<ul>
<li><p><a href="http://stanford.edu/~boyd/cvxbook/"><i>Convex Optimization</i></a>, by S. Boyd and L. Vandenberghe, Cambridge University Press, 2003.</p>
</li>
</ul>
<p><b>References:</b></p>
<ul>
<li><p><a href="http://users.iems.northwestern.edu/~nocedal/book/"><i>Numerical Optimization</i></a>, by J. Nocedal and S. Wright, Springer-Verlag, 2006.</p>
</li>
<li><p><a href="http://www.athenasc.com/nonlinbook.html"><i>Nonlinear Programming</i></a>,  by D. Bertsekas, Athena Scientific. 2016.</p>
</li>
<li><p><a href="http://bookstore.siam.org/mo25/"><i>First-order Methods in Optimization</i></a>, by A. Beck, MOS-SIAM Series on Optimization, 2017.</p>
</li>
<li><p><a href="https://www.cambridge.org/core/books/optimization-methods-in-finance/8A4996C5DB2006224E4D983B5BC95E3B"><i>Optimization Methods in Finance</i></a>, by G. Cornuejols, J. Pena, and R. Tutuncu, Cambridge University Press, 2018.</p>
</li>
<li><p><a href="https://www.nowpublishers.com/article/Details/SIG-072"><i>A Signal Processing Perspective on Financial Engineering</i></a>, by Yiyong Feng and Daniel P. Palomar, Foundations and Trends in Signal Processing, Now Publishers, 2016.</p>
</li>
<li><p><a href="https://www.cambridge.org/core/books/highdimensional-probability/797C466DA29743D2C8213493BD2D2102"><i>High-Dimensional Probability: An Introduction with Applications in Data Science</i></a>, by Roman Vershynin, Cambridge University Press, 2018.</p>
</li>
<li><p><a href="https://www.cambridge.org/core/books/highdimensional-statistics/8A91ECEEC38F46DAB53E9FF8757C7A4E"><i>High-Dimensional Statistics: A Non-Asymptotic Viewpoint</i></a>, by Martin Wainwright, Cambridge University Press, 2019.</p>
</li>
<li><p><a href="http://www.nowpublishers.com/article/Details/MAL-058"><i>Non-convex Optimization for Machine Learning</i></a>, by P. Jain and P. Kark, Foundations and Trends in Machine Learning, 2017.</p>
</li>
</ul>
<h2>Lectures</h2>
<ol>
<li><p><b>Theoretical foundations</b></p>
<ol>
<li><p>Convex sets</p>
</li>
<li><p>Convex functions</p>
</li>
<li><p>Convex optimization problems</p>
</li>
<li><p>Lagrange duality and KKT conditions</p>
</li>
<li><p>Disciplined convex programming</p>
</li></ol>
</li>
<li><p><b>First-order methods</b></p>
<ol>
<li><p>Gradient methods</p>
</li>
<li><p>Subgradient methods</p>
</li>
<li><p>Proximal methods</p>
</li></ol>
</li>
<li><p><b>Second-order methods</b></p>
<ol>
<li><p>Newton Method</p>
</li>
<li><p>Interior-point methods</p>
</li>
<li><p>Quasi-Newton methods</p>
</li></ol>
</li>
<li><p><b>Stochastic methods</b></p>
<ol>
<li><p>Stochastic gradient methods</p>
</li>
<li><p>Stochastic Newton methods</p>
</li>
<li><p>Randomized sketching methods</p>
</li></ol>
</li>
<li><p><b>Applications in data science</b></p>
<ol>
<li><p>Convex geometry: phase transitions</p>
</li>
<li><p>Global geometry: saddle point characterizations</p>
</li>
<li><p>Local geometry: basin of attraction</p>
</li>
</ol>

</li>
</ol>
<div id="footer">
<div id="footer-text">
Page generated 2019-02-01 16:21:09 CST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
(<a href="cvx.jemdoc">source</a>)
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
